# -*- coding: utf-8 -*-
"""Linear_sum2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPUiqlc2Dmpgj0k2t6krNNbFRiLkT7gE
"""

import matplotlib.pyplot as plt

class LinearRegression:
    def __init__(self, learning_rate=0.01, epochs=1000, tolerance=1e-6):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.tolerance = tolerance  # Stop early if gradient updates are very small
        self.weights = None
        self.bias = 0
        self.loss_history = []  # Store MSE loss for visualization
        self.history = []  # Store weight and bias updates

    def mse(self, y, y_hat):
        error = sum((y_hat[i] - y[i]) ** 2 for i in range(len(y)))
        return error / len(y)

    def compute_y_hat(self, x):
        return sum(x[i] * self.weights[i] for i in range(len(x))) + self.bias

    def compute_gradients(self, X, y):
        n_samples = len(X)
        n_features = len(X[0])
        dw = [0] * n_features
        db = 0

        for j in range(n_samples):
            y_pred = self.compute_y_hat(X[j])
            error = y[j] - y_pred
            for i in range(n_features):
                dw[i] += error * X[j][i]
            db += error

        dw = [-(2 / n_samples) * dwi for dwi in dw]
        db *= -(2 / n_samples)

        return dw, db

    def descent(self, dw, db):
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * dw[i]
        self.bias -= self.learning_rate * db

    def fit(self, X, y):
        n_features = len(X[0])
        self.weights = [0] * n_features
        prev_dw, prev_db = [float('inf')] * n_features, float('inf')  # Track previous gradients

        for epoch in range(self.epochs):
            dw, db = self.compute_gradients(X, y)

            # Simple early stopping: Check if gradient change is small
            if all(abs(dw[i] - prev_dw[i]) < self.tolerance for i in range(n_features)) and abs(db - prev_db) < self.tolerance:
                print(f'Converged at epoch {epoch}')
                break

            self.descent(dw, db)
            prev_dw, prev_db = dw, db  # Store previous gradients

            # Store weight, bias, and loss for visualization
            y_pred = self.predict(X)
            loss = self.mse(y, y_pred)
            self.loss_history.append(loss)
            self.history.append((self.weights[:], self.bias))

    def predict(self, X):
        return [self.compute_y_hat(x) for x in X]

    def visualize_training(self, X, y):
        plt.figure(figsize=(12, 5))

        # Plot loss over epochs
        plt.subplot(1, 2, 1)
        plt.plot(range(len(self.loss_history)), self.loss_history, color='red')
        plt.xlabel("Epochs")
        plt.ylabel("MSE Loss")
        plt.title("Loss Over Iterations")

        # Plot regression line evolution
        plt.subplot(1, 2, 2)
        x_vals = [x[0] for x in X]  # Extract feature values
        plt.scatter(x_vals, y, color='blue', label='Actual Data')

        for i, (weights, b) in enumerate(self.history[::max(1, len(self.history)//10)]):
            y_vals = [weights[0] * x + b for x in x_vals]
            plt.plot(x_vals, y_vals, color='gray', alpha=0.3, linestyle='dashed')

        # Final regression line
        y_final = [self.weights[0] * x + self.bias for x in x_vals]
        plt.plot(x_vals, y_final, color='red', label='Final Fit')

        plt.xlabel("Feature Value")
        plt.ylabel("Target Value")
        plt.title("Regression Line Evolution")
        plt.legend()

        plt.show()

#2
X = [[100], [200], [300], [400], [500], [600], [700], [800], [900], [1000]]
y = [1050, 1100, 1150, 1200, 1250, 1300, 1350, 1400, 1450, 1500]  # y = 0.5x + 1000


exp1=LinearRegression(learning_rate=0.000001,epochs=200)
exp1.fit(X, y)
exp1.visualize_training(X, y)
#learning rate 0.025 was too high, therefore had to use a lower lr, it is hilarious seeing the equation fail when using multiple learning rates

exp2=LinearRegression(learning_rate=0.03,epochs=200)
exp2.fit(X, y)
exp2.visualize_training(X, y)

exp3=LinearRegression(learning_rate=0.000000005,epochs=200)
exp3.fit(X, y)
exp3.visualize_training(X, y)

exp4=LinearRegression(learning_rate=0.0000025,epochs=50000)
exp4.fit(X, y)
exp4.visualize_training(X, y)

#2.2
#data with noise, does it work?
X2 = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]
y2= [5, 10, 15, 18, 21, 26, 30, 34, 37, 43]  # y = 4x + 2 with slight variations
exp6=LinearRegression(learning_rate=0.0001,epochs=100000)
exp6.fit(X2, y2)
exp6.visualize_training(X2, y2)

#2.2
#test train manual experimentation
X3 = [[11],[12],[-1],[-2]]
exp6.predict(X3)
#very accurate results close enough to 4x+2

