{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5u3FYqSGzeN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "\n",
        "class logistic:\n",
        "  def __init__(self,weights,bias,epochs,learning_rate,tolerance):\n",
        "    self.weights=[]\n",
        "    self.bias=0\n",
        "    self.epochs=epochs\n",
        "    self.learning_rate=learning_rate\n",
        "    self.tolerance=tolerance\n",
        "\n",
        "  def huber_loss(self,y,y_hat,delta=1):\n",
        "    loss=0\n",
        "    for i in range(len(y)):\n",
        "      error=y_hat[i]-y[i]\n",
        "      if abs(error<delta):\n",
        "        loss+=0.5*(error**2)\n",
        "      else:\n",
        "        loss+=delta*(abs(error)-0.5*delta)\n",
        "    return loss/len(y)\n",
        "\n",
        "  def compute_y_hat(self,x):\n",
        "    return sum(x[i]*self.weights[i] for i in range(len(self.weights)) + self.bias)\n",
        "\n",
        "  def compute_gradient(self,X,y):\n",
        "    n_samples=len(X)\n",
        "    n_features=len(X[0])\n",
        "    dw=[0]*n_features\n",
        "    db=0\n",
        "\n",
        "    for j in range(n_samples):\n",
        "      y_pred=self.compute_y_hat[j]\n",
        "      error=y_pred-y[i]\n",
        "      for i in range(n_features):\n",
        "        dw[i]+=error*X[j][i]\n",
        "      db+=error\n",
        "    dw = [dw_i / n_samples for dw_i in dw]\n",
        "    db /= n_samples\n",
        "\n",
        "    return dw,db\n",
        "\n",
        "  def descent(self, dw, db):\n",
        "    for i in range(len(self.weights)):\n",
        "      self.weights[i] -= self.learning_rate * dw[i]\n",
        "    self.bias -= self.learning_rate * db\n",
        "\n",
        "  def sigmoid(self, z):\n",
        "        return 1 / (1 + math.exp(-z))\n",
        "\n",
        "  def fit(self,X,y):\n",
        "    prev_loss=float('-inf')\n",
        "    for epoch in range(self.epochs):\n",
        "      y_hat=[self.compute_y_hat(X[i]) for i in range (len(X))]\n",
        "      loss=self.huber_loss(y,y_hat)\n",
        "      dw,db=self.compute_gradient(X,y)\n",
        "      self.descent(dw,db)\n",
        "\n",
        "      if abs(prev_loss - loss) < self.tolerance:\n",
        "                print(\"Converged\")\n",
        "                break\n",
        "\n",
        "      prev_loss=loss\n",
        "\n",
        "  def predict(self, X):\n",
        "        return [self.compute_y_hat(x) for x in X]\n",
        "\n",
        "  def visualize(self, X, y):\n",
        "        \"\"\"Visualize the logistic regression decision boundary and sigmoid function.\"\"\"\n",
        "        x_values = np.linspace(-10, 10, 100)\n",
        "        sigmoid_values = [self.sigmoid(x) for x in x_values]\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # Plot sigmoid function\n",
        "        plt.plot(x_values, sigmoid_values, label=\"Sigmoid Function\", color=\"blue\")\n",
        "\n",
        "        # Plot decision boundary (linear function before sigmoid)\n",
        "        linear_x_values = np.linspace(min([x[0] for x in X]), max([x[0] for x in X]), 100)\n",
        "        decision_boundary = [- (self.bias + self.weights[0] * x) / self.weights[1] for x in linear_x_values]\n",
        "\n",
        "        # Plot data points\n",
        "        X_data = np.array(X)\n",
        "        y_data = np.array(y)\n",
        "\n",
        "        plt.scatter(X_data[:, 0], y_data, color='red', label=\"Data Points\")\n",
        "\n",
        "        # Titles and labels\n",
        "        plt.axhline(0.5, color=\"gray\", linestyle=\"--\", label=\"Decision Boundary\")\n",
        "        plt.xlabel(\"Feature X\")\n",
        "        plt.ylabel(\"Sigmoid Output\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Logistic Regression Visualization\")\n",
        "        plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1o4L48-lIyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}